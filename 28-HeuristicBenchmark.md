The Heuristic Benchmark pattern compares an ML model against a simple, easy-to-understand heuristic in order to explain the model’s performance to business decision makers. 

A good heuristic benchmark should be intuitively easy to understand and relatively trivial to compute. If we find ourselves defending or debugging the algorithm used by the benchmark, we should search for a simpler, more understandable one. Good examples of a heuristic benchmark are constants, rules of thumb, or bulk statistics (such as the mean, median, or mode). Avoid the temptation to train even a simple machine learning model, such as a linear regression, on a dataset and use that as a benchmark linear regression is likely not intuitive enough, especially once we start to include categorical variables, more than a handful of inputs, or engineered features.

Human Experts - Using human experts need not be limited to unstructured data as with eye disease or damage cost estimation. For example, if we are building a model to predict whether or not a loan will get refinanced within a year, the data will be tabular and the ground truth will be available in the historical data. However, even in this case, we might ask human experts to identify loans that will get refinanced for the purposes of communicating how often loan agents in the field would get it right. 

Utility Value - it is important to translate the improvement in model performance into the model’s utility value. This value could be monetary, but it could also correspond with other measures of utility, like better search results, earlier disease detection, or less waste resulting from improved manufacturing efficiency. This utility value is useful in deciding whether or not to deploy this model, since deploying or changing a production model always carries a certain cost in terms of reliability and error budgets. 

